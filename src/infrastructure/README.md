# ğŸ–¥ï¸ Infrastructure â€” Configuration & Logging

> La couche **infrastructure** gÃ¨re la configuration YAML hiÃ©rarchique et le logging structurÃ©.
> Elle fournit les paramÃ¨tres Ã  toutes les autres couches.

---

## ğŸ—ï¸ Vue d'ensemble

```mermaid
graph TB
    subgraph "ğŸ–¥ï¸ Infrastructure"
        CFG["âš™ï¸ Config<br/><i>config.rs</i><br/>Configuration YAML"]
        LOG["ğŸ“œ Logging<br/><i>logging.rs</i><br/>tracing + EnvFilter"]
    end

    subgraph "ğŸ“„ Sources de config"
        YML["ğŸ“„ config.yaml<br/><i>Fichier par dÃ©faut</i>"]
        ENV["ğŸŒ Variables d'env<br/><i>SPEC_FORGE_*</i>"]
        CLI["ğŸ–¥ï¸ Arguments CLI<br/><i>clap overrides</i>"]
    end

    YML --> CFG
    ENV --> CFG
    CLI --> CFG

    CFG --> APP["âš™ï¸ Application"]
    CFG --> ADP["ğŸ”§ Adapters"]
    LOG --> ALL["ğŸ“Š Tous les modules"]

    style CFG fill:#2196F3,stroke:#333,color:#fff
    style LOG fill:#4CAF50,stroke:#333,color:#fff
    style YML fill:#FF9800,stroke:#333,color:#fff
```

---

## ğŸ“ Fichiers

| Fichier | RÃ´le | Taille |
|---------|------|--------|
| âš™ï¸ `config.rs` | Chargement et validation de la configuration YAML | ~17 Ko |
| ğŸ“œ `logging.rs` | Initialisation de `tracing` avec filtre par niveau | ~1 Ko |

---

## âš™ï¸ Configuration â€” Sections

```mermaid
graph LR
    CFG["âš™ï¸ Config"]
    CFG --> PIP["ğŸ”„ Pipeline<br/><i>max_retries, language,<br/>token_budget</i>"]
    CFG --> LLM["ğŸ¤– LLM<br/><i>provider, model,<br/>temperature, context</i>"]
    CFG --> OUT["ğŸ“¤ Output<br/><i>spec_format,<br/>gherkin_language</i>"]
    CFG --> VAL["âœ… Validation<br/><i>coverage %, syntax,<br/>max_clarifications</i>"]
    CFG --> LOGG["ğŸ“œ Logging<br/><i>level, format,<br/>colors</i>"]

    style CFG fill:#2196F3,stroke:#333,color:#fff
    style PIP fill:#FF9800,stroke:#333,color:#fff
    style LLM fill:#4CAF50,stroke:#333,color:#fff
    style OUT fill:#9C27B0,stroke:#333,color:#fff
```

### ğŸ“‹ ParamÃ¨tres clÃ©s

| Section | ParamÃ¨tre | DÃ©faut | Description |
|---------|-----------|--------|-------------|
| `llm` | `provider` | `ollama` | Provider LLM |
| `llm` | `model_name` | `qwen2.5:7b` | ModÃ¨le Ã  utiliser |
| `llm` | `temperature` | `0.1` | CrÃ©ativitÃ© (0.0â€“1.0) |
| `llm` | `context_size` | `8192` | FenÃªtre de contexte (tokens) |
| `pipeline` | `max_retries` | `3` | Tentatives max par appel LLM |
| `pipeline` | `default_language` | `fr` | Langue par dÃ©faut |
| `output` | `gherkin_language` | `fr` | Mots-clÃ©s Gherkin (fr/en) |
| `validation` | `min_coverage_percent` | `80` | Couverture minimale exigÃ©e |
| `logging` | `level` | `info` | Niveau de log |

---

## ğŸ“œ Logging â€” tracing

Le systÃ¨me utilise `tracing` avec `EnvFilter` pour un logging structurÃ© :

```bash
# Ajuster le niveau de log
RUST_LOG=debug cargo run -- pipeline -i input.md -o output/

# Niveaux disponibles : error, warn, info, debug, trace
```
