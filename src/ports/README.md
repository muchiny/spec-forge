# ğŸ”Œ Ports â€” Interfaces abstraites

> Les **ports** dÃ©finissent les contrats (traits) entre le domaine/application et le monde extÃ©rieur.
> Chaque port est un trait Rust avec `#[async_trait]` + `Send + Sync`.

---

## ğŸ—ï¸ Vue d'ensemble

```mermaid
graph LR
    subgraph "ğŸ”Œ Ports (Traits)"
        P1["ğŸ§  LlmService"]
        P2["ğŸ“¥ InputReader"]
        P3["ğŸ“¤ OutputWriter"]
        P4["ğŸ“ TemplateEngine"]
    end

    subgraph "ğŸ”§ Adapters (ImplÃ©mentations)"
        A1["OllamaAdapter<br/>MockAdapter"]
        A2["MarkdownReader<br/>YamlReader<br/>PdfReader<br/>DocxReader"]
        A3["MarkdownWriter<br/>GherkinWriter<br/>TraceabilityWriter"]
        A4["FileTemplateEngine"]
    end

    P1 -.->|"impl"| A1
    P2 -.->|"impl"| A2
    P3 -.->|"impl"| A3
    P4 -.->|"impl"| A4

    style P1 fill:#2196F3,stroke:#333,color:#fff
    style P2 fill:#2196F3,stroke:#333,color:#fff
    style P3 fill:#2196F3,stroke:#333,color:#fff
    style P4 fill:#2196F3,stroke:#333,color:#fff
    style A1 fill:#FF9800,stroke:#333,color:#fff
    style A2 fill:#FF9800,stroke:#333,color:#fff
    style A3 fill:#FF9800,stroke:#333,color:#fff
    style A4 fill:#FF9800,stroke:#333,color:#fff
```

---

## ğŸ“ Fichiers

| Fichier | Trait | ResponsabilitÃ© |
|---------|-------|----------------|
| ğŸ§  `llm_service.rs` | `LlmService` | Communication avec le LLM (generate, check_connection) |
| ğŸ“¥ `input_reader.rs` | `InputReader` | Lecture des User Stories depuis un fichier |
| ğŸ“¤ `output_writer.rs` | `OutputWriter` | Ã‰criture des artefacts gÃ©nÃ©rÃ©s |
| ğŸ“ `template_engine.rs` | `TemplateEngine` | Chargement et rendu des templates de prompts |

---

## ğŸ§  LlmService â€” Le port principal

```mermaid
sequenceDiagram
    participant App as âš™ï¸ Application
    participant Port as ğŸ”Œ LlmService
    participant Adapter as ğŸ”§ OllamaAdapter
    participant LLM as ğŸ¤– Ollama

    App->>Port: generate(prompt, config)
    Port->>Adapter: HTTP POST /api/generate
    Adapter->>LLM: RequÃªte JSON
    LLM-->>Adapter: RÃ©ponse JSON
    Adapter-->>Port: LlmResponse
    Port-->>App: Result<LlmResponse>
```

### ğŸ“¦ Types associÃ©s

| Type | Description |
|------|-------------|
| `LlmConfig` | Configuration (model, temperature, max_tokens, context_size) |
| `LlmResponse` | RÃ©ponse LLM (response, model, finish_reason) |
| `FinishReason` | Raison d'arrÃªt (Stop, Length, Error) |
| `LlmError` | Erreurs LLM (ConnectionFailed, ModelNotFound, GenerationFailed) |

---

## ğŸ”‘ Principe d'injection de dÃ©pendances

```rust
// âœ… Injection via Arc<dyn Trait>
let llm: Arc<dyn LlmService> = Arc::new(OllamaAdapter::new(config));
let pipeline = Pipeline::new(llm, reader, writer, template_engine);
```

> ğŸ’¡ **Avantage** : les tests utilisent `MockAdapter` sans modifier le code applicatif.

---

## â• Ajouter un nouveau port

1. CrÃ©er `src/ports/mon_port.rs` avec un trait `#[async_trait]`
2. Ajouter `pub mod mon_port;` dans `src/ports/mod.rs`
3. CrÃ©er l'adapter correspondant dans `src/adapters/`
4. Injecter via `Arc<dyn MonPort>` dans l'application
